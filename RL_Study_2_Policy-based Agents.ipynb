{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart-Pole Task\n",
    "In this tutorial, we will learn how to train a pole to balance itself, which is also a typical reinforce learning problem\n",
    "\n",
    "In order to accomplish this, we are going to need a challenge that is more difficult for the agent than the two-armed bandit. To meet provide this challenge we are going to utilize the OpenAI gym, a collection of reinforcement learning environments. We will be using one of the classic tasks, the Cart-Pole. To learn more about the OpenAI gym, and this specific task, check out their tutorial here. Essentially, we are going to have our agent learn how to balance a pole for as long as possible without it falling. Unlike the two-armed bandit, this task requires:<br>\n",
    "\n",
    "> **Observations** — The agent needs to know where pole currently is, and the angle at which it is balancing. To accomplish this, our neural network will take an observation and use it when producing the probability of an action.<br>\n",
    "**Delayed reward** — Keeping the pole in the air as long as possible means moving in ways that will be advantageous for both the present and the future. To accomplish this we will adjust the reward value for each observation-action pair using a function that weighs actions over time.<br>\n",
    "\n",
    "To take reward over time into account, the form of Policy Gradient we used in the previous tutorials will need a few adjustments. The first of which is that we now need to update our agent with more than one experience at a time. To accomplish this, we will collect experiences in a buffer, and then occasionally use them to update the agent all at once. These sequences of experience are sometimes referred to as rollouts, or experience traces. We can’t just apply these rollouts by themselves however, we will need to ensure that the rewards are properly adjusted by a discount factor.\n",
    "\n",
    "Intuitively this allows each action to be a little bit responsible for not only the immediate reward, but all the rewards that followed. We now use this modified reward as an estimation of the advantage in our loss equation. With those changes, we are ready to solve CartPole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range\n",
    "    \n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Policy-Based Agent\n",
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and computer discounted reward\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    \n",
    "    for t in reversed(xrange(0,r.size)):\n",
    "        # Cumulate the reward\n",
    "        runnning_add = running_add * gamma + r[t]\n",
    "        # add the reward to certain action\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # THe agent take a state and performs an action based on that\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size], dtype= tf.float32)\n",
    "        hidden      = slim.fully_connected(self.state_in, h_size, biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden, a_size, biases_initializer=None, activation_fn=tf.nn.softmax)\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # define reward and action tensor \n",
    "        self.reward_holder = tf.placeholder(shape=[None], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None], dtype =tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_output = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_output)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        \n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "            \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawn/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0\n",
      "38.3\n",
      "55.66\n",
      "101.91\n",
      "156.24\n",
      "166.71\n",
      "172.67\n",
      "188.94\n",
      "186.27\n",
      "184.18\n",
      "179.82\n",
      "168.14\n",
      "190.52\n",
      "194.49\n",
      "187.19\n",
      "180.64\n",
      "185.49\n",
      "186.99\n",
      "190.5\n",
      "195.79\n",
      "199.17\n",
      "199.93\n",
      "198.85\n",
      "200.0\n",
      "198.94\n",
      "197.88\n",
      "199.78\n",
      "198.07\n",
      "197.35\n",
      "196.67\n",
      "195.14\n",
      "196.26\n",
      "189.35\n",
      "189.89\n",
      "194.77\n",
      "194.49\n",
      "192.86\n",
      "192.78\n",
      "194.33\n",
      "196.55\n",
      "194.77\n",
      "191.71\n",
      "194.41\n",
      "198.07\n",
      "198.16\n",
      "190.17\n",
      "180.27\n",
      "185.99\n",
      "191.26\n",
      "187.37\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # Reset the graph \n",
    "\n",
    "myAgent = agent(lr =1e-2, s_size=4, a_size=2, h_size=8) # initialize an agent with 4 states, 2 action and 8 hidden layers\n",
    "\n",
    "total_epoch = 5000 # Set total number of episodes\n",
    "max_ep = 999 # TODO change back to 9990\n",
    "update_frequency = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph \n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    i = 0 \n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "    \n",
    "    # trainable_variables: \n",
    "    # [<tf.Variable 'fully_connected/weights:0' shape=(4, 4) dtype=float32_ref>,\n",
    "    #  <tf.Variable 'fully_connected_1/weights:0' shape=(4, 2) dtype=float32_ref>]\n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0 \n",
    "\n",
    "    while i < total_epoch:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        \n",
    "        for j in range(max_ep):\n",
    "            # Probabilistically pick an action given our network output.\n",
    "            a_dist = sess.run(myAgent.output, feed_dict={myAgent.state_in:[s]})\n",
    "            # a is a vector with two number sum to 1 \n",
    "            a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "            # randomly pick a number from a \n",
    "            a = np.argmax(a_dist == a)\n",
    "            # then return the index of the larger num\n",
    "\n",
    "            s1,r,d,_ = env.step(a) # Get our rewar for taking an action given a bandit\n",
    "            ep_history.append([s,a,r,s1]) # Record \n",
    "            s = s1 # update state\n",
    "            running_reward += r # culmulate reward \n",
    "            \n",
    "            if d == True:\n",
    "                # Update the network\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2]) #Update reward\n",
    "                feed_dict = {myAgent.reward_holder:ep_history[:,2],\n",
    "                            myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict) #calculate the gradient of loss\n",
    "                \n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "                    \n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict = dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0 \n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "                \n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-100:]))\n",
    "\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
